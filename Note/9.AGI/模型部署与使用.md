### 模型部署与使用笔记

#### 1. vLLM 部署命令
```bash
# 部署 DeepSeek-Qwen2.5-7B 模型
export HF_ENDPOINT=https://hf-mirror.com
vllm serve /home/aistudio/script/model/deepseek-Qwen2.5-7B/ \
  --served-model-name Qwen2.5:0.5B \
  --max-model-len 16384 \
  --max-num-seqs 2 \
  --gpu-memory-utilization 0.8 \
  --tensor-parallel-size 2 \
  --enforce-eager \
  --swap-space=1

# 部署 QwQ-32B-AWQ 模型
vllm serve .cache/modelscope/hub/models/qwen/QwQ-32B-AWQ \
  --served-model-name QwQ:32B \
  --max-model-len 4096
```

#### 2. API 调用示例
```bash
# 使用 Completions 接口
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "QwQ:32B",
    "prompt": "2019第十届上海国际冷冻冷藏食品博览会，这个展会简称是什么？",
    "max_tokens": 2048,
    "temperature": 0.6,
    "top_p": 0.95,
    "top_k": 40,
    "repetition_penalty": 1.1
  }'

# 使用 Chat Completions 接口
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5:0.5B",
    "messages": [
      {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},
      {"role": "user", "content": "深圳举办的展会有哪些？"}
    ],
    "temperature": 0,
    "top_p": 0.8,
    "repetition_penalty": 1.05,
    "max_tokens": 512
  }'
```

#### 3. 模型下载方法
```bash
# 使用 HuggingFace CLI 下载 GGUF 格式
huggingface-cli download Qwen/QwQ-32B-GGUF \
  --include "qwq-32b-q5_k_m-00006-of-00006.gguf" \
  --local-dir . \
  --local-dir-use-symlinks False

# 使用 ModelScope 下载 AWQ 格式
pip install modelscope
modelscope download --model="qwen/QwQ-32B-AWQ"
```

#### 4. GGUF 模型处理
```bash
# 合并分割的 GGUF 文件
./llama.cpp/build/bin/llama-gguf-split --merge \
  model/qwq-32b-q5_k_m-00001-of-00006.gguf \
  model/qwq-32b-q5_k_m.gguf

# 使用 llama.cpp 部署
./llama.cpp/build/bin/llama-cli -m qwq-32b-q5_k_m.gguf

# 使用 vLLM 部署 GGUF
vllm serve ./qwq-32b-q5_k_m.gguf \
  --served-model-name QwQ:32B \
  --max-model-len 6384 \
  --max-num-seqs 2 \
  --gpu-memory-utilization 0.8
```

#### 5. Unsloth 使用注意事项
- **Qwen2.5-7B 在 16GB GPU 上继续预训练**
  ```python
  from unsloth import FastLanguageModel
  model, tokenizer = FastLanguageModel.from_pretrained(
      model_name = "unsloth/Qwen2.5-7B",  # 非官方支持
      max_seq_length = 2048,
      dtype = None,
      load_in_4bit = True,  # 必须启用 4bit 量化
  )
  ```
  **常见问题**：
  - OOM 错误解决方案：
    1. 减少 `max_seq_length`（尝试 1024）
    2. 使用更小的批处理大小
    3. 启用梯度检查点：`model.gradient_checkpointing_enable()`
    4. 使用 QLoRA 适配器而非全参数训练

#### 6. 提示工程示例
```json
{
  "prompt": "问：深圳举办的展会有哪些？\n答：？",
  "prompt": "2015第十二届中国国际烘焙展览会 举办时间？",
  "instruction": "下面是一条描述任务的指令，与提供上下文的输入配对。写一个匹配请求的输出。\n\n### Input:\n 深圳举办的展会有哪些？### Response:{}"
}
```

---

### 关键参数说明
| 参数 | 说明 |
|------|------|
| `--tensor-parallel-size` | 张量并行度（多卡分割） |
| `--max-model-len` | 模型最大上下文长度 |
| `--gpu-memory-utilization` | GPU 显存利用率阈值 |
| `--enforce-eager` | 禁用 CUDA Graph 优化 |
| `--swap-space` | CPU 交换空间大小 (GB) |
| `top_k` + `top_p` | 采样策略控制输出多样性 |
| `repetition_penalty` | 重复惩罚系数 (>1.0 抑制重复) |

> **注意**：Qwen2.5 架构在 Unsloth 中的支持可能不完善，建议优先使用 Hugging Face 原生方法进行微调。AWQ/GGUF 格式适用于资源受限环境，但会损失部分精度。