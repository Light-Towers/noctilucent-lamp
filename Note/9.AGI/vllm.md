# è½»æ¾ä¸Šæ‰‹vLLMï¼šå¤§è¯­è¨€æ¨¡å‹æ¨ç†åŠ é€Ÿçš„å®ç”¨æŒ‡å—

## vLLMæ˜¯ä»€ä¹ˆï¼Ÿ

**vLLM**ï¼ˆVery Large Language Modelï¼‰æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è®¾è®¡çš„é«˜æ€§èƒ½å¼€æºæ¡†æ¶ï¼Œé€šè¿‡åˆ›æ–°çš„PagedAttentionæŠ€æœ¯å¤§å¹…æå‡æ¨ç†é€Ÿåº¦å’Œæ˜¾å­˜åˆ©ç”¨ç‡ï¼Œè®©å¤§æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­è·‘å¾—æ›´å¿«ã€æ›´çœèµ„æºã€‚

### æ ¸å¿ƒä¼˜åŠ¿

- **æé«˜æ€§èƒ½**ï¼šæ¯”HuggingFace Transformerså¿«24å€ï¼Œæ¯”Text Generation Inferenceå¿«3.5å€
- **æ˜¾å­˜ä¼˜åŒ–**ï¼šPagedAttentionæŠ€æœ¯å®ç°96%ä»¥ä¸Šçš„æ˜¾å­˜åˆ©ç”¨ç‡
- **æ— ç¼å…¼å®¹**ï¼šæ”¯æŒHuggingFaceæ¨¡å‹ï¼Œå…¼å®¹OpenAI APIæ¥å£
- **æ˜“äºéƒ¨ç½²**ï¼šæ”¯æŒå¤šGPUåˆ†å¸ƒå¼æ¨ç†ï¼Œç®€åŒ–å¤§æ¨¡å‹éƒ¨ç½²

## å¿«é€Ÿå®‰è£…

```bash
# åŸºç¡€å®‰è£…
pip install vllm

# å¦‚æœéœ€è¦GPUæ”¯æŒï¼ˆæ¨èï¼‰
pip install 'vllm[gpu]'

# æˆ–è€…ä»æºç å®‰è£…æœ€æ–°ç‰ˆæœ¬
pip install git+https://github.com/vllm-project/vllm.git
```

## åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

### 1. æœ€ç®€å•çš„æ¨ç†ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ä½œä¸ºç¤ºä¾‹ï¼‰
llm = LLM(model="facebook/opt-125m")

# è®¾ç½®ç”Ÿæˆå‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,    # æ§åˆ¶éšæœºæ€§
    top_p=0.95,        # æ ¸é‡‡æ ·å‚æ•°
    max_tokens=100     # æœ€å¤§ç”Ÿæˆé•¿åº¦
)

# å‡†å¤‡è¾“å…¥
prompts = [
    "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯",
    "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    "Pythonç¼–ç¨‹çš„ä¼˜åŠ¿åœ¨äº"
]

# æ‰¹é‡ç”Ÿæˆ
outputs = llm.generate(prompts, sampling_params)

# è¾“å‡ºç»“æœ
for i, output in enumerate(outputs):
    print(f"è¾“å…¥ {i+1}: {output.prompt}")
    print(f"ç”Ÿæˆç»“æœ: {output.outputs[0].text}")
    print("-" * 50)
```

### 2. ä¸­æ–‡æ¨¡å‹æ¨ç†ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

# ä½¿ç”¨ä¸­æ–‡æ¨¡å‹ï¼ˆéœ€è¦å…ˆä¸‹è½½æ¨¡å‹ï¼‰
llm = LLM(model="THUDM/chatglm2-6b")

# è®¾ç½®é€‚åˆä¸­æ–‡çš„å‚æ•°
sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.8,
    max_tokens=200,
    stop=["<|endoftext|>", "<|im_end|>"]  # è®¾ç½®åœæ­¢è¯
)

# ä¸­æ–‡å¯¹è¯ç¤ºä¾‹
prompts = [
    "è¯·ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
    "å¦‚ä½•å­¦ä¹ Pythonç¼–ç¨‹ï¼Ÿ",
    "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸæœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"é—®é¢˜: {output.prompt}")
    print(f"å›ç­”: {output.outputs[0].text}")
    print("=" * 60)
```

### 3. æµå¼è¾“å‡ºç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

llm = LLM(model="facebook/opt-125m")
sampling_params = SamplingParams(temperature=0.8, max_tokens=150)

# æµå¼ç”Ÿæˆ
prompt = "è¯·è¯¦ç»†è§£é‡Šä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯"

# æ³¨æ„ï¼švLLMçš„æµå¼è¾“å‡ºéœ€è¦ç‰¹æ®Šé…ç½®
outputs = llm.generate([prompt], sampling_params)

for output in outputs:
    print("å®Œæ•´å›ç­”:")
    print(output.outputs[0].text)
```

## é«˜çº§åŠŸèƒ½ä½¿ç”¨

### 1. å¤šGPUåˆ†å¸ƒå¼æ¨ç†

```python
from vllm import LLM, SamplingParams

# é…ç½®å¤šGPU
llm = LLM(
    model="facebook/opt-125m",
    tensor_parallel_size=2,  # ä½¿ç”¨2ä¸ªGPU
    gpu_memory_utilization=0.8  # GPUæ˜¾å­˜ä½¿ç”¨ç‡
)

sampling_params = SamplingParams(temperature=0.7, max_tokens=100)
outputs = llm.generate(["å¤šGPUæ¨ç†æµ‹è¯•"], sampling_params)
print(outputs[0].outputs[0].text)
```

### 2. è‡ªå®šä¹‰é‡‡æ ·ç­–ç•¥

```python
from vllm import LLM, SamplingParams

llm = LLM(model="facebook/opt-125m")

# ä¸åŒçš„é‡‡æ ·ç­–ç•¥
strategies = {
    "ä¿å®ˆç­–ç•¥": SamplingParams(temperature=0.1, top_p=0.5, max_tokens=50),
    "å¹³è¡¡ç­–ç•¥": SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100),
    "åˆ›æ„ç­–ç•¥": SamplingParams(temperature=1.2, top_p=0.95, max_tokens=150)
}

prompt = "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"

for strategy_name, params in strategies.items():
    print(f"\n{strategy_name}:")
    outputs = llm.generate([prompt], params)
    print(outputs[0].outputs[0].text)
```

### 3. æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
from vllm import LLM, SamplingParams
import time

llm = LLM(model="facebook/opt-125m")

# å‡†å¤‡å¤§é‡è¾“å…¥
prompts = [f"è¯·è§£é‡Šç¬¬{i}ä¸ªæ¦‚å¿µ" for i in range(1, 21)]

sampling_params = SamplingParams(temperature=0.7, max_tokens=50)

# æ‰¹é‡å¤„ç†
start_time = time.time()
outputs = llm.generate(prompts, sampling_params)
end_time = time.time()

print(f"å¤„ç†äº†{len(prompts)}ä¸ªè¯·æ±‚ï¼Œè€—æ—¶: {end_time - start_time:.2f}ç§’")
print(f"å¹³å‡æ¯ä¸ªè¯·æ±‚: {(end_time - start_time)/len(prompts):.3f}ç§’")
```

## APIæœåŠ¡éƒ¨ç½²

### 1. å¯åŠ¨OpenAIå…¼å®¹APIæœåŠ¡

```bash
# åŸºç¡€å¯åŠ¨
python -m vllm.entrypoints.openai.api_server \
    --model facebook/opt-125m \
    --port 8000

# é«˜çº§é…ç½®å¯åŠ¨
python -m vllm.entrypoints.openai.api_server \
    --model facebook/opt-125m \
    --port 8000 \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 2048

# --trust-remote-code  # æœ‰äº›æ¨¡å‹åŒ…å«è‡ªå®šä¹‰ä»£ç ï¼Œå…è®¸æ‰§è¡Œè‡ªå®šä¹‰ä»£ç 
# --enforce-eager           # å…³é”®å‚æ•°1ï¼šå¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼/PyTorchåŸç”Ÿæ¨¡å¼ï¼Œè·³è¿‡ç®—å­èåˆä¼˜åŒ–ï¼ˆå…¶ä¸­åŒ…å«FAï¼‰
# --disable-custom-kernels    # å…³é”®å‚æ•°2ï¼šç¦ç”¨æ‰€æœ‰è‡ªå®šä¹‰å†…æ ¸ï¼ˆåŒ…æ‹¬FlashAttentionï¼‰
python -m vllm.entrypoints.openai.api_server \
    --model jinaai/jina-embeddings-v4-vllm-retrieval \
    --served-model-name jinaai/jina-embeddings-v4-vllm-retrieval \
    --max-model-len 8768 \
    --trust-remote-code \
    --enforce-eager \
    --disable-custom-all-reduce \
    --gpu-memory-utilization 0.9


```

### 2. å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹

```python
import requests
import json

# APIè°ƒç”¨ç¤ºä¾‹
def call_vllm_api(prompt, temperature=0.7, max_tokens=100):
    url = "http://localhost:8000/generate"
    
    data = {
        "prompt": prompt,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "n": 1
    }
    
    response = requests.post(url, json=data)
    result = response.json()
    
    return result["text"][0]

# ä½¿ç”¨ç¤ºä¾‹
result = call_vllm_api("è¯·ä»‹ç»ä¸€ä¸‹vLLMæ¡†æ¶çš„ä¼˜åŠ¿")
print(result)
```

### 3. ä½¿ç”¨OpenAIå®¢æˆ·ç«¯åº“

```python
from openai import OpenAI

# é…ç½®å®¢æˆ·ç«¯
client = OpenAI(
    api_key="EMPTY",  # vLLMä¸éœ€è¦APIå¯†é’¥
    base_url="http://localhost:8000/v1"
)

# è°ƒç”¨API
response = client.completions.create(
    model="facebook/opt-125m",
    prompt="è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
    max_tokens=100,
    temperature=0.7
)

print(response.choices[0].text)
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 1. æ˜¾å­˜ä¼˜åŒ–é…ç½®

```python
from vllm import LLM, SamplingParams

# ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
llm = LLM(
    model="facebook/opt-125m",
    gpu_memory_utilization=0.9,  # æé«˜æ˜¾å­˜åˆ©ç”¨ç‡
    max_model_len=2048,          # é™åˆ¶æœ€å¤§åºåˆ—é•¿åº¦
    swap_space=4,                # è®¾ç½®äº¤æ¢ç©ºé—´
    cpu_offload_gb=2             # CPUå¸è½½æ˜¾å­˜
)
```

### 2. æ‰¹å¤„ç†ä¼˜åŒ–

```python
# æ‰¹é‡å¤§å°ä¼˜åŒ–
llm = LLM(
    model="facebook/opt-125m",
    max_num_batched_tokens=4096,  # æ‰¹å¤„ç†tokenæ•°é‡
    max_num_seqs=256              # æœ€å¤§å¹¶å‘åºåˆ—æ•°
)
```

### 3. æ¨¡å‹é‡åŒ–

```python
# ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘æ˜¾å­˜å ç”¨
llm = LLM(
    model="facebook/opt-125m",
    quantization="awq",  # æˆ– "gptq", "squeezellm"
    dtype="half"         # ä½¿ç”¨åŠç²¾åº¦
)
```

## å¸¸è§é—®é¢˜è§£å†³

### 1. æ˜¾å­˜ä¸è¶³

```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘æ˜¾å­˜ä½¿ç”¨
llm = LLM(
    model="facebook/opt-125m",
    gpu_memory_utilization=0.6,  # é™ä½æ˜¾å­˜ä½¿ç”¨ç‡
    max_model_len=1024,          # å‡å°‘æœ€å¤§é•¿åº¦
    cpu_offload_gb=4             # å¢åŠ CPUå¸è½½
)
```

### 2. æ¨¡å‹åŠ è½½å¤±è´¥

```python
# è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæƒé™
llm = LLM(
    model="facebook/opt-125m",
    trust_remote_code=True,      # ä¿¡ä»»è¿œç¨‹ä»£ç 
    download_dir="./models"      # æŒ‡å®šä¸‹è½½ç›®å½•
)
```

### 3. å¤šGPUé…ç½®é—®é¢˜

```python
# è§£å†³æ–¹æ¡ˆï¼šæ­£ç¡®é…ç½®å¼ é‡å¹¶è¡Œ
llm = LLM(
    model="facebook/opt-125m",
    tensor_parallel_size=2,      # ç¡®ä¿GPUæ•°é‡åŒ¹é…
    pipeline_parallel_size=1,    # æµæ°´çº¿å¹¶è¡Œ
    distributed_executor_backend="ray"  # ä½¿ç”¨Rayåç«¯
)
```

## å®é™…åº”ç”¨åœºæ™¯

### 1. æ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
from vllm import LLM, SamplingParams

class ChatBot:
    def __init__(self, model_name):
        self.llm = LLM(model=model_name)
        self.sampling_params = SamplingParams(
            temperature=0.7,
            max_tokens=200,
            stop=["ç”¨æˆ·:", "ç³»ç»Ÿ:"]
        )
    
    def chat(self, user_input):
        prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
        outputs = self.llm.generate([prompt], self.sampling_params)
        return outputs[0].outputs[0].text

# ä½¿ç”¨ç¤ºä¾‹
bot = ChatBot("facebook/opt-125m")
response = bot.chat("ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ ä»¬çš„äº§å“")
print(response)
```

### 2. å†…å®¹ç”ŸæˆæœåŠ¡

```python
def generate_content(content_type, topic):
    llm = LLM(model="facebook/opt-125m")
    
    prompts = {
        "æ–‡ç« ": f"è¯·å†™ä¸€ç¯‡å…³äº{topic}çš„æ–‡ç« ",
        "æ‘˜è¦": f"è¯·ä¸ºä»¥ä¸‹å†…å®¹å†™æ‘˜è¦: {topic}",
        "æ ‡é¢˜": f"è¯·ä¸º{topic}ç”Ÿæˆ5ä¸ªå¸å¼•äººçš„æ ‡é¢˜"
    }
    
    sampling_params = SamplingParams(temperature=0.8, max_tokens=300)
    outputs = llm.generate([prompts[content_type]], sampling_params)
    
    return outputs[0].outputs[0].text

# ä½¿ç”¨ç¤ºä¾‹
article = generate_content("æ–‡ç« ", "äººå·¥æ™ºèƒ½çš„å‘å±•")
print(article)
```

## æ€»ç»“

vLLMæ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ä¸”æ˜“äºä½¿ç”¨çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œé€šè¿‡PagedAttentionæŠ€æœ¯å®ç°äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚æ— è®ºæ˜¯ç®€å•çš„æ–‡æœ¬ç”Ÿæˆè¿˜æ˜¯å¤æ‚çš„å¤šGPUåˆ†å¸ƒå¼éƒ¨ç½²ï¼ŒvLLMéƒ½æä¾›äº†ç®€æ´çš„APIå’Œä¸°å¯Œçš„é…ç½®é€‰é¡¹ã€‚

**å…³é”®ä¼˜åŠ¿**ï¼š

- ğŸš€ **æ€§èƒ½å“è¶Š**ï¼šæ¯”ä¼ ç»Ÿæ¡†æ¶å¿«æ•°å€
- ğŸ’¾ **æ˜¾å­˜é«˜æ•ˆ**ï¼š96%ä»¥ä¸Šçš„æ˜¾å­˜åˆ©ç”¨ç‡
- ğŸ”§ **æ˜“äºä½¿ç”¨**ï¼šç®€å•çš„Python API
- ğŸŒ **éƒ¨ç½²çµæ´»**ï¼šæ”¯æŒAPIæœåŠ¡å’Œåˆ†å¸ƒå¼éƒ¨ç½²

é€šè¿‡æœ¬æŒ‡å—çš„ç¤ºä¾‹ä»£ç ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿä¸Šæ‰‹vLLMï¼Œåœ¨å®é™…é¡¹ç›®ä¸­äº«å—å¤§æ¨¡å‹æ¨ç†çš„åŠ é€Ÿæ•ˆæœï¼




# TODO