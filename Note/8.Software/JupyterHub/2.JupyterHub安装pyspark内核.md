# JupyterHub安装pyspark内核

## 安装python依赖包

pyspark需要依赖的包：

```shell
pip3 install ez_setup
pip3 install setuptools_rust
pip3 install --upgrade setuptools
pip3 install setuptools_scm
pip3 install wheel
pip3 install cffi
pip3 install jupyterlab

# 安装 ipykernel
python3 -m pip install ipykernel
python3 -m ipykernel install --user

python3 -m pip install pyspark
python3 -m pip install pyspark-kernel
python3 -m pip install ipython

```

## 配置

为了让你的 pyspark 在 yarn 模式下工作，你必须做一些额外的配置：

1. 将 yarn 集群的 hadoop-yarn-server-web-proxy-<version>.jar 复制到 <local hadoop directory>/hadoop-<version>/share/hadoop/yarn/（你需要一个本地 hadoop）
2. 将集群的 hive-site.xml 复制到 <local spark directory>/spark-<version>/conf/
3. 将集群的 yarn-site.xml 复制到 <local hadoop directory>/hadoop-<version>/hadoop-<version>/etc/hadoop/
4. 设置 `environment` 环境变量
   * `export HADOOP_HOME=<local hadoop directory>/hadoop-<version>`
   * `export SPARK_HOME=<local spark directory>/spark-<version>`
   * `export HADOOP_CONF_DIR=<local hadoop directory>/hadoop-<version>/etc/hadoop`
   * `export YARN_CONF_DIR=<local hadoop directory>/hadoop-<version>/etc/hadoop`

5. 现在，您可以创建内核`kernel.json`文件

   * 在 conda kernels 目录下新建pyspark文件夹

   ```shell
   cd /opt/conda/anaconda3/share/jupyter/kernels/
   mkdir pyspark && cd pyspark
   ```

   * `vim kernel.json`

	`PYSPARK_SUBMIT_ARGS`参数需加上hive配置`--conf spark.sql.catalogImplementation=hive`，否则无法查到数据
	
	```json
	{
	 "argv": [
	  "/opt/conda/anaconda3/bin/python3",
	  "-m",
	  "pyspark_kernel",
	  "-f",
	  "{connection_file}"
	 ],
	 "display_name": "PySpark",
	 "language": "python",
	 "env": {
	      "PYSPARK_PYTHON": "/opt/conda/anaconda3/bin/python3",
	      "SPARK_HOME": "/usr/lib/spark-current",
	      "PYTHONPATH": "/usr/lib/spark-current/python/:/usr/lib/spark-current/python/lib/py4j-0.10.9-src.zip",
	      "PYTHONSTARTUP": "/usr/lib/spark-current/python/pyspark/shell.py",
	      "HADOOP_CONF_DIR": "/etc/ecm/hadoop-conf",
	      "PYSPARK_SUBMIT_ARGS": "--master yarn --deploy-mode client --conf spark.sql.catalogImplementation=hive --driver-memory 4g --num-executors 4 --executor-memory 4g --executor-cores 4 pyspark-shell"
	  }
	}
	```

6. 重启 jupyterhub



## kernel的一些操作

* 查看所有的kernel

​	jupyter kernelspec list

* 删除指定的kernel

​	jupyter kernelspec remove 'kernelname'



## 参考

[Can a PySpark Kernel(JupyterHub) run in yarn-client mode?](https://stackoverflow.com/questions/41105726/can-a-pyspark-kerneljupyterhub-run-in-yarn-client-mode)

