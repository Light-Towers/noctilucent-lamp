# 模型部署与使用综合笔记

> **注意**：有关 vLLM 的详细部署和使用教程，请查看 [vllm.md](./vllm.md) 文件。

---

## 1. Unsloth 使用指南

### 1.1 Qwen2.5-7B 在 16GB GPU 上继续预训练
```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Qwen2.5-7B",  # 非官方支持
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,  # 必须启用 4bit 量化
)
```

### 1.2 常见问题与解决方案
- **OOM（显存溢出）错误**：
  1. **减少序列长度**：将 `max_seq_length` 从 2048 减至 1024
  2. **减小批处理大小**：使用更小的批处理规模
  3. **启用梯度检查点**：调用 `model.gradient_checkpointing_enable()`
  4. **使用QLoRA适配器**：替代全参数训练，大幅减少显存占用
  5. **启用梯度累积**：积累多个小批次的梯度再更新

- **训练速度慢**：
  1. **优化数据加载**：使用预加载和缓存
  2. **调整优化器**：尝试不同的学习率和优化器配置
  3. **硬件检查**：确保GPU驱动和CUDA版本兼容

---

## 2. llama.cpp 模型处理

### 2.1 GGUF 文件合并
```bash
# 合并分割的 GGUF 文件
./llama.cpp/build/bin/llama-gguf-split --merge \
  model/qwq-32b-q5_k_m-00001-of-00006.gguf \
  model/qwq-32b-q5_k_m.gguf
```

**注意事项**：
- GGUF 文件通常被分割为多个部分以便下载
- 使用 `llama-gguf-split` 工具进行合并
- 确保所有分割文件在同一目录下

### 2.2 使用 llama.cpp 部署模型
```bash
# 基础推理
./llama.cpp/build/bin/llama-cli -m qwq-32b-q5_k_m.gguf

# 指定上下文长度和线程数
./llama.cpp/build/bin/llama-cli \
  -m qwq-32b-q5_k_m.gguf \
  -n 512 \           # 生成token数量
  -c 4096 \          # 上下文长度
  -t 8 \             # 线程数
  -p "问题："        # 提示词
```

### 2.3 llama.cpp 常用参数
| 参数 | 说明 | 示例值 |
|------|------|--------|
| `-m` | 模型文件路径 | `model.gguf` |
| `-n` | 生成token数量 | `512` |
| `-c` | 上下文长度 | `4096` |
| `-t` | CPU线程数 | `8` |
| `-p` | 提示词 | `"请解释："` |
| `--temp` | 温度参数 | `0.7` |
| `--top-p` | top-p采样 | `0.9` |
| `--repeat-penalty` | 重复惩罚 | `1.1` |

---

## 3. 提示工程最佳实践

### 3.1 基础提示模板
```json
{
  "system_prompt": "你是一个专业的AI助手，回答要准确、简洁。",
  "user_prompt": "深圳举办的展会有哪些？",
  "response_format": "请列出展会名称、时间和地点"
}
```

### 3.2 具体应用示例
```json
{
  "问答格式": "问：深圳举办的展会有哪些？\n答：？",
  
  "特定查询": "2015第十二届中国国际烘焙展览会 举办时间？",
  
  "指令格式": "下面是一条描述任务的指令，与提供上下文的输入配对。写一个匹配请求的输出。\n\n### Input:\n 深圳举办的展会有哪些？\n### Response:{}",
  
  "分步推理": "请按步骤分析：\n1. 识别问题类型\n2. 提取关键信息\n3. 搜索相关知识\n4. 组织答案结构\n5. 输出最终答案"
}
```

### 3.3 提示优化技巧
1. **明确角色**：为AI分配特定角色（专家、助手、顾问等）
2. **结构化输出**：要求特定格式（列表、表格、JSON等）
3. **分步思考**：鼓励链式推理过程
4. **提供示例**：给出少量示例（few-shot learning）
5. **长度控制**：指定答案的最大/最小长度

---

## 4. 通用部署注意事项

### 4.1 模型格式选择
| 格式 | 优点 | 缺点 | 适用场景 |
|------|------|------|----------|
| **原生HF** | 精度最高，功能最全 | 显存占用大 | 研究、全参数微调 |
| **GGUF** | 内存高效，CPU友好 | 精度损失 | 资源受限环境、CPU推理 |
| **AWQ** | 精度保持较好，速度快 | 需要支持硬件 | 生产环境、GPU推理 |
| **GPTQ** | 压缩率高，速度快 | 需要特殊工具 | 显存有限场景 |

### 4.2 硬件配置建议
- **16GB GPU**：
  - Qwen2.5-7B（4bit量化）
  - 较小模型的微调任务
  - 使用QLoRA适配器

- **24GB+ GPU**：
  - 13B模型全参数微调
  - 7B模型的高精度训练
  - 多任务并行推理

- **CPU部署**：
  - 使用GGUF格式
  - 选择较低量化级别（Q4_K_M, Q5_K_M）
  - 利用多线程加速

### 4.3 性能优化策略
1. **量化选择**：根据精度要求选择合适量化级别
2. **批处理优化**：调整批处理大小平衡吞吐和延迟
3. **内存管理**：合理设置交换空间和显存利用率
4. **并行处理**：利用多GPU或多CPU核心

---

## 总结

### 关键建议
1. **工具选择**：
   - 研究/微调：优先使用Hugging Face + Unsloth
   - 生产部署：考虑vLLM或llama.cpp
   - 资源受限：选择GGUF格式+llama.cpp

2. **格式兼容性**：
   - Qwen2.5在Unsloth中的支持可能不完善
   - AWQ/GGUF格式适用于资源受限环境，但会损失部分精度
   - 生产环境建议测试多种部署方案

3. **持续学习**：
   - 关注工具版本更新
   - 测试新功能和优化策略
   - 根据实际需求调整配置

> **最后提醒**：不同工具链有各自优势，选择最适合您具体需求的方案。建议在实际部署前进行充分的测试和验证。
