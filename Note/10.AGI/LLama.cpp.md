# llama.cpp 使用笔记：关键要点与常见陷阱

## 一、核心概念与环境准备

### ✅ 正确认知
1. **llama.cpp 定位**：
   - 纯 C++ 的高效推理框架
   - 支持模型层卸载（offload）到 GPU
   - 依赖 GGUF 量化格式模型

2. **硬件与驱动要求**：
   - 需要 NVIDIA GPU 和对应驱动
   - CUDA 环境必须正确配置
   - 通过 `nvidia-smi` 验证环境

3. **关键参数理解**：
   - `-ngl`：控制卸载到 GPU 的模型层数
   - `-m`：指定 GGUF 模型文件路径
   - `-c`：设置上下文长度
   - `-n`：限制生成 token 数量

## 二、安装与编译流程

### ✅ 正确步骤
```bash
# 1. 获取源代码
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# 2. 启用 CUDA 编译（关键选项）
mkdir build && cd build
cmake .. -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Release
make -j$(nproc)

# 3. 确认生成的可执行文件
cd bin && ls -lh llama-cli llama-server
```

### ⚠️ 编译常见错误
| 错误现象 | 原因 | 解决方案 |
|----------|------|----------|
| `LLAMA_CUBLAS is deprecated` | 使用过时编译选项 | 改用 `-DGGML_CUDA=ON` |
| `main not found` | 程序名称变更 | 寻找 `llama-cli` 或 `llama-server` |
| CUDA 未找到 | 环境问题 | 检查 `nvidia-smi` 和 CUDA 安装 |

## 三、模型运行实践

### ✅ 基础命令结构
```bash
# 基本格式
./llama-cli -m <模型路径> -p "<提示词>" -n <生成长度> -ngl <GPU层数>

# 完整示例
./llama-cli -m ~/models/model.gguf -p "解释深度学习" -n 300 -ngl 999 -c 4096
```

### ✅ 参数使用要点
1. **GPU 卸载策略**：
   - `-ngl 0`：纯 CPU 推理
   - `-ngl 999`：尝试全部层卸载到 GPU
   - 实际卸载层数在日志中显示

2. **输出控制**：
   - `--color on/off/auto`：控制颜色输出（需指定值）
   - `--no-interactive`：禁用交互模式
   - `--silent-prompt`：隐藏提示词显示

## 四、实际问题与解决方案

### ❌ 遇到的问题与解决
1. **模型功能混淆**
   - 现象：期望对话但模型是专用型（如 Reranker）
   - 对策：测试简单任务确认模型能力，阅读模型文档

2. **提示词格式错误**
   ```bash
   # 错误示例（续行符后加注释）
   -n 5 \  # 生成5个token  # 导致解析失败
   
   # 多行提示词的正确处理
   -p $'第一行\n第二行'     # 使用 $'...' 语法
   --file prompt.txt       # 或使用文件输入
   ```

3. **意外进入交互模式**
   - 现象：输出后等待用户输入
   - 解决：添加 `--no-interactive` 或确保有 `-n` 参数

### ✅ 验证方法
1. **GPU 加速确认**：
   ```
   ggml_cuda_init: found X CUDA devices
   llm_load_tensors: offloaded X/Y layers to GPU
   ```

2. **性能监控**：
   ```bash
   # GPU 状态
   nvidia-smi
   # llama.cpp 性能输出
   [ Generation: 250.5 t/s ]  # tokens/秒
   ```

## 五、模型类型区分与使用

### 🔄 不同模型类型对比
| 模型类型 | 主要功能 | 输出形式 | 使用场景 |
|----------|----------|----------|----------|
| **对话模型** | 文本生成、问答 | 连贯文本 | 聊天、创作、分析 |
| **Reranker 模型** | 相关性评分 | 数值分数 | 检索系统精排 |
| **Embedding 模型** | 文本向量化 | 向量数组 | 语义搜索、聚类 |

### 📝 专用模型使用要点
1. **Reranker 模型需要结构化输入**：
   ```
   查询：[问题文本]
   文档：[待评分文档]
   得分：
   ```

2. **明确模型能力边界**：
   - 测试简单任务确认能力
   - 查看模型来源和文档
   - 调整提示词格式匹配训练数据

## 六、核心经验总结

### 🎯 关键工作流程
1. **环境验证**：CUDA → 驱动 → 编译选项
2. **模型测试**：小任务 → 逐步复杂化 → 确认能力
3. **参数调整**：GPU 卸载 → 上下文长度 → 生成控制

### 💡 实用建议
1. **首次运行策略**：
   - 从量化小模型开始测试
   - 逐步增加复杂度
   - 监控资源使用情况

2. **问题排查路径**：
   ```
   环境验证 → 编译检查 → 模型验证 → 参数调试 → 提示词优化
   ```

3. **资源管理意识**：
   - 监控 GPU 显存使用
   - 注意上下文长度对资源的影响
   - 根据任务需求调整批次大小

### ⚠️ 必须避免的错误
1. **假设所有 GGUF 模型功能相同**
2. **在 bash 续行符后直接添加注释**
3. **忽略模型卡片中的特殊说明**
4. **混淆不同模型的预期输出格式**

### ✅ 最佳实践
1. **编译时**：始终使用 `-DGGML_CUDA=ON` 启用 GPU 支持
2. **运行时**：通过日志确认 GPU 卸载实际层数
3. **调试时**：先用最小可复现代例定位问题
4. **部署时**：根据模型类型设计合适的提示词模板
