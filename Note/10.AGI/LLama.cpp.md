# llama.cpp 使用笔记：关键要点、实战与硬件匹配

## 一、核心概念与环境准备

### ✅ 正确认知
1.  **llama.cpp 定位**：
    *   纯 C++ 的高效推理框架
    *   支持模型层卸载（offload）到 GPU
    *   依赖 GGUF 量化格式模型

2.  **硬件与驱动要求**：
    *   需要 NVIDIA GPU 和对应驱动
    *   CUDA 环境必须正确配置
    *   通过 `nvidia-smi` 验证环境

3.  **关键参数理解**：
    *   `-ngl` (`--n-gpu-layers`)：控制卸载到 GPU 的模型层数，是性能调优的关键。
    *   `-m`：指定 GGUF 模型文件路径。
    *   `-c` (`--ctx-size`)：设置上下文长度，影响内存占用和“记忆”能力。
    *   `-n` (`--n-predict`)：限制生成 token 数量。
    *   `--threads`：设置用于计算的CPU线程数，可设为 `$(nproc)` 使用所有逻辑核心。
    *   `-ot` (`--override-tensor`)：**高级参数**，用于覆盖特定张量的计算设备（如 `CPU` 或 `CUDA0`），对控制MoE模型行为至关重要。

## 二、安装与编译流程

### ✅ 正确步骤
```bash
# 1. 获取源代码
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# 2. 启用 CUDA 编译（关键选项，注意历史变更）
mkdir build && cd build
cmake .. -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Release  # 使用 GGML_CUDA 而非已废弃的 LLAMA_CUBLAS
make -j$(nproc)

# 3. 确认生成的可执行文件
cd bin && ls -lh llama-cli llama-server  # 主程序名可能为 llama-cli 而非 main
```

### ⚠️ 编译常见错误
| 错误现象 | 原因 | 解决方案 |
| :--- | :--- | :--- |
| `LLAMA_CUBLAS is deprecated` | 使用过时编译选项 | 改用 `-DGGML_CUDA=ON` |
| `main not found` | 程序名称变更 | 寻找 `llama-cli` 或 `llama-server` |
| CUDA 未找到 | 环境问题 | 检查 `nvidia-smi` 和 CUDA 安装 |

## 三、模型运行实践

### ✅ 基础命令结构
```bash
# 基础对话模型
./llama-cli -m <模型路径> -p "<提示词>" -n <生成长度> -ngl <GPU层数>

# 启动API服务器
./llama-server -m <模型路径> --host 0.0.0.0 --port <端口> -ngl <GPU层数>

# 示例
./llama-server \
  -m ~/models/Qwen3-Next-80B-A3B-Instruct-UD-TQ1_0.gguf \
  --host 0.0.0.0 \
  --port 11435 \
  --no-webui \
  -c 8192 \
  -ngl 999 
  -ot ".ffn_gate.=CUDA0" -ot ".ffn_up.=CPU" -ot ".ffn_down.=CPU" \    # **关键：卸载FFN层到CPU**     # -ot ".ffn_.*_exps.=CPU" \  # **关键：卸载MoE专家层到CPU**
  --threads $(nproc)
```

### ✅ 参数使用要点
1.  **GPU 卸载策略**：
    *   `-ngl 0`：纯 CPU 推理。
    *   `-ngl 999`：尝试全部层卸载到 GPU（适用于显存充足的小模型）。
    *   `-ngl N`：混合模式，首N层在GPU，其余在CPU（**有限显存运行大模型的必需策略**）。
    *   实际卸载层数需在日志 `llm_load_tensors: offloaded X/Y layers to GPU` 中确认。

2.  **输出与控制**：
    *   `--color on/off/auto`：控制颜色输出（**必须指定值**，如 `--color on`）。
    *   `--no-interactive`：禁用交互模式，处理单次请求后退出。
    *   `--silent-prompt`：隐藏提示词显示。

3.  **性能与资源**：
    *   `-c 4096`：设置上下文长度，值越大占用资源越多。
    *   `--threads 8`：设置CPU计算线程数。
    *   `--no-warmup`：跳过启动时的空运行预热，加速启动。

## 四、高级功能与模型实战

### 🧩 MoE大模型实战：以 Qwen3-Next-80B 为例
在**有限显存（如32GB）上运行超大规模MoE模型**需要特殊策略，核心矛盾是**显存无法容纳全部参数**。

| 挑战 | 策略 | 命令关键参数 |
| :--- | :--- | :--- |
| **总参数量庞大** | 使用深度量化模型（如 `UD-TQ1_0`） | `-m model.gguf` |
| **无法全载入显存** | **混合计算**：基础层放GPU，专家层卸载到CPU | `-ngl 40 -ot ".ffn_.*_exps.=CPU"` |
| **速度优化** | 在显存上限内，尽可能增加GPU层数 | 逐步增加 `-ngl` 值直到显存将满 |
| **KV缓存失效** | 确保部分层在GPU以启用缓存 | 成功的混合模式会使 `timings.cache_n > 0` |

**典型工作流程**：
1.  **从保守参数开始**：`-ngl 20 -ot “.ffn_.*_exps.=CPU” -c 1024`
2.  **监控与调优**：启动后使用 `nvidia-smi` 监控显存，逐步增加 `-ngl`。
3.  **性能验证**：通过API请求测试，关注响应时间和 `timings` 中的 `predicted_per_second`（生成速度）。

### 🔄 模型类型区分与使用
| 模型类型 | 主要功能 | 输出形式 | 使用场景 |
| :--- | :--- | :--- | :--- |
| **对话模型** | 文本生成、问答 | 连贯文本 | 聊天、创作、分析 |
| **Reranker 模型** | 相关性评分 | 数值分数 | 检索系统精排，需特定提示格式 |
| **Embedding 模型** | 文本向量化 | 向量数组 | 语义搜索、聚类 |

### ⚠️ 实际问题与解决方案
1.  **模型功能混淆**：
    *   **现象**：期望对话但模型是专用型（如 Reranker）。
    *   **对策**：测试简单任务确认模型能力，阅读模型文档。

2.  **提示词格式错误**：
    ```bash
    # 错误：续行符后直接加注释
    -n 5 \  # 生成5个token  # 会导致解析失败
    
    # 正确：处理多行提示词
    -p $'第一行\n第二行'     # 使用 $'...' 语法
    --file prompt.txt       # 或使用文件输入
    ```

3.  **GPU未利用或利用率低**：
    *   **现象**：监控显示GPU计算占用率为0%，但CPU满载。
    *   **诊断**：
        *   检查启动日志确认 `offloaded X/Y layers to GPU`。
        *   对于MoE模型，使用 `-ot` 将专家层卸载到CPU是正常策略，GPU可能仅负责基础层和存储。
    *   **验证**：发送真实推理请求，观察GPU占用率是否有波动。

## 五、性能监控与调试

### 📊 关键监控指标
1.  **GPU状态**：使用 `nvidia-smi` 监控**显存占用**和**GPU计算利用率**。
2.  **系统资源**：使用 `htop`、`free` 监控CPU和内存占用。
3.  **API性能**：响应JSON中的 `timings` 字段，核心指标：
    *   `predicted_per_second`：生成速度（tokens/秒），**衡量可用性的关键**。
    *   `cache_n`：KV缓存命中数，大于0表示优化生效。

### 🐞 调试流程
```
现象 → 检查服务健康 (`curl /health`) → 查看启动日志 → 监控资源使用 → 调整参数 → 重复测试
```

## 六、核心经验总结

### 🎯 关键工作流程
1.  **环境验证**：CUDA → 驱动 → 编译选项。
2.  **模型测试**：小任务 → 逐步复杂化 → 确认能力与类型。
3.  **参数调整**：GPU卸载 (`-ngl`) → 上下文长度 (`-c`) → 线程控制 (`--threads`) → 高级卸载 (`-ot`)。

### 💡 硬件与模型匹配的黄金法则
| 你的硬件 (GPU显存) | 推荐的模型参数规模 | 预期体验 | 关键配置 |
| :--- | :--- | :--- | :--- |
| **8GB** | **7B** 量化模型 | 流畅 | `-ngl 999`，全GPU运行 |
| **24GB** | **13B-34B** 量化模型 | 流畅-良好 | `-ngl 999` 或 `-ngl` 高位 |
| **32GB (V100)** | **70B+ MoE** 模型 | **极慢，仅可验证** | **必须**使用 `-ngl` 与 `-ot` 混合计算 |
| **32GB (V100)** | **7B/14B** 量化模型 | **极快且流畅** | `-ngl 999`，充分发挥硬件性能 |

**来自实战的重要结论**：在单张V100 32GB上运行Qwen3-Next-80B-A3B-Instruct模型，即使经过最优调参，生成速度的**理论上限约为60 tokens/秒**，无法满足流畅交互的体验。**追求实用性时，应选择与显存匹配的模型规模。**

### ✅ 最佳实践
1.  **编译时**：始终使用 `-DGGML_CUDA=ON` 启用 GPU 支持。
2.  **运行时**：通过日志确认 GPU 卸载实际层数，使用 `nvidia-smi` 监控显存。
3.  **调试时**：先用最小可复现代例定位问题，每次只调整一个参数。
4.  **部署时**：根据模型类型设计合适的提示词模板，MoE模型需接受混合计算模式。

### ⚠️ 必须避免的错误
1.  假设所有 GGUF 模型功能相同。
2.  在 bash 续行符 (`\`) 后直接添加注释。
3.  忽略模型卡片中的特殊说明（如专用格式、量化类型）。
4.  混淆不同模型的预期输出格式。
5.  **在显存有限的情况下，强行尝试用 `-ngl 999` 运行远超显存容量的模型**。应优先采用混合计算或换用更小模型。